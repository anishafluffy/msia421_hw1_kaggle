#Description stats
summary(dat)
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
art_collect
View(art_collect)
result2 = tapply(dat$qty, dat$category,mean)
sort_q = result2[order(result2)]
sort_q
View(dat)
View(dat)
result3 = tapply(dat$qty, dat$category,median)
sort_q3 = result3[order(result3)]
sort_q3
result4 = tapply(dat$qty, dat$category,max)
sort_q4 = result4[order(result4)]
sort_q4
result = tapply(dat$price, dat$category,max)
sort_price = result[order(result)]
sort_price
qp = tapply(dat$qty * dat$price, dat$category,mean)
sort_qp = qp[order(qp)]
sort_qp
missing = dat[!complete.cases(dat),] #no missing value
#a.category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
#category by price (most $)
result = tapply(dat$price, dat$category,mean)
sort_price = result[order(result)] #category 17 (art prints) is $$$
art_collect = dat[dat$category==17,] #these people buy at most 3 items
#category by Q (most popular category)
result2 = tapply(dat$qty, dat$category,mean)
sort_q = result2[order(result2)] #note - cat99 nonbooks has the highest avg
result3 = tapply(dat$qty, dat$category,median)
sort_q3 = result3[order(result3)] #median is all 1
result4 = tapply(dat$qty, dat$category,max)
sort_q4 = result4[order(result4)] #category 99 is nonbooks, ID 8070857 has price =0, Q = max.
#category by price * Q (most popular category)
qp = tapply(dat$qty * dat$price, dat$category,mean)
sort_qp = qp[order(qp)] #align with expectation - 17 has the largest avg order size
qpm = tapply(dat$qty * dat$price, dat$category,max)
sort_qpm = qpm[order(qpm)]
sort_qpm
result2
as.Date?
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
#Descriptive stats
summary(dat)
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
#Descriptive stats
summary(dat[,-1])
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
#Descriptive stats
summary(dat[,-1])
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
head(x)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/siliangchen/msia421_hw1_kaggle")
library(ggplot2)
library(lubridate)
library(MASS)
library(dplyr)
library(car)
#read orders
dat = read.csv("data/orders.csv")
dat$orddt = as.Date(dat$orddate, "%d%b%Y")
head(dat)
dat$orddate = NULL
str(dat)
dim(dat)
missing = dat[!complete.cases(dat),] #no missing value
#a.category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
#category by price (most $)
result = tapply(dat$price, dat$category,mean)
sort_price = result[order(result)] #category 17 (art prints) is $$$
art_collect = dat[dat$category==17,] #these people buy at most 3 items
#category by Q (most popular category)
result2 = tapply(dat$qty, dat$category,mean)
sort_q = result2[order(result2)] #note - cat99 nonbooks has the highest avg
result3 = tapply(dat$qty, dat$category,median)
sort_q3 = result3[order(result3)] #median is all 1
result4 = tapply(dat$qty, dat$category,max)
sort_q4 = result4[order(result4)] #category 99 is nonbooks, ID 8070857 has price =0, Q = max.
#category by price * Q (most popular category)
qp = tapply(dat$qty * dat$price, dat$category,mean)
sort_qp = qp[order(qp)] #align with expectation - 17 has the largest avg order size
qpm = tapply(dat$qty * dat$price, dat$category,max)
sort_qpm = qpm[order(qpm)] #8,14,35,37 have the max one-time order amounts, >$140k; makes sense, 37 is learning=> textbooks
#Descriptive stats
summary(dat[,-1])
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
dplyr::select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
#avg order size
x$avg_ord = x$monetary_tot/x$frequency_ord
cor_mat = cor(x[-1])
#purchase rate = purchases/period
x$prate = x$frequency_ord/x$recency_first
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
#avg order size
x$avg_ord = x$monetary_tot/x$frequency_ord
#purchase rate = purchases/period
x$prate = x$frequency_ord/x$recency_first
#check predictors cor
cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
# read in dependent variable
y = read.csv("data/booktrain.csv")
#head(y)
#join tables
all = left_join(x,y,by="id")
dim(all)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
#plot(sqrt(train$frequency_qty), train$logtarg)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot((train$frequency_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
#plot(sqrt(train$frequency_qty), train$logtarg)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
p_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
dplyr::select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
date_duration = recency_last - recency_first, #time between 1st and last purchases
p_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
dplyr::select(id, recency_first, recency_last, date_duration, p_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
#avg order size
x$avg_ord = x$monetary_tot/x$frequency_ord
#purchase rate = purchases/period
x$prate = x$frequency_ord/x$recency_first
#check predictors cor
cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected, avg_ord and monetary_tot
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
plot((train$p_qty), train$logtarg)
# read in dependent variable
y = read.csv("data/booktrain.csv")
#head(y)
#join tables
all = left_join(x,y,by="id")
dim(all)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
plot((train$p_qty), train$logtarg)
plot(sqrt(train$p_qty), train$logtarg)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
plot((train$p_qty), train$logtarg)
plot(log(train$p_qty), train$logtarg)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
plot((train$p_qty), train$logtarg)
plot(log(train$p_qty), train$logtarg)
plot(train$prate,train$logtarg)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)
plot((train$p_qty), train$logtarg)
plot(log(train$p_qty), train$logtarg)
plot(log(train$prate),train$logtarg)
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
names(all)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(p_qty) + log(prate) + recency_first + recency_last, all)
summary(fit2)
#vif(fit2)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(monetary_tot +1) + log(avg_ord+1) + log(frequency_ord) + log(p_qty) + log(prate) + recency_first + recency_last, all)
summary(fit2)
#vif(fit2)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(monetary_tot +1) + log(avg_ord+1) + log(frequency_ord) + log(p_qty) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
fit2 = lm(logtarg ~ log(monetary_tot +1) + log(avg_ord+1) + log(frequency_ord) + log(p_qty) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(p_qty) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(p_qty) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
all$yhat = predict(fit2,all)
length(all$yhat)
head(all)
View(all)
abs(-1)
all$yhat = abs(predict(fit2,all))
length(all$yhat)
head(all)
count(all[,'yhat']<0)
all['yhat'<0]
View(all)
all[,'yhat']
count(all[,'yhat']<0)
len(all[,'yhat'<0])
len(all[,'yhat']<0)
yhat = all[,'yhat']<0
View(y)
yhat = all[,c(12,13)]
View(yhat)
all$yhat = predict(fit2,all)
#Take absolute value bc there are a small proportion of yhat with negative values
all$yhat = predict(fit2,all)
length(all$yhat<0)
head(all)
#Take absolute value bc there are a small proportion of yhat with negative values
all$yhat = predict(fit2,all)
length(all$yhat)
head(all)
View(all)
#Predict and output
all$yhat = predict(fit2,all)
length(all$yhat)
head(all)
#output test values
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
write.csv(out, "output/test_jc.csv", row.names=F)
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(prate) + recency_first + recency_last, all)
summary(fit2)
vif(fit2)
#Predict and output
all$yhat = predict(fit2,all)
length(all$yhat)
head(all)
#output test values
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
write.csv(out, "output/test_jc.csv", row.names=F)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/siliangchen/Desktop/Northwestern/Coursework/M421 Data mining")
np2 = read.table(np2.dat,header=T)
np2 = read.table("np2.dat",header=T)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/siliangchen/Desktop/Northwestern/Coursework/M421 Data mining/Clustering")
np2 = read.table("np2.dat",header=T)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/siliangchen/Desktop/Northwestern/Coursework/M421 Data mining/Clustering")
np2 = read.table("np2.dat",header=T)
View(np2)
View(np2)
np2 = read.table("np2.dat",header=T)
npxlab = "Number hours/week"
npylab = "Number sections"
#make image/contour plot
library(askima)
install.packages("askima")
install.packages("akima")
np2 = read.table("np2.dat",header=T)
npxlab = "Number hours/week"
npylab = "Number sections"
#make image/contour plot
library(akima)
np2.smooth = with(np2, interp(time, sections, sqrt(count), x0=seq(0,7,length=60), yo=seq(0,7,length=60)))
np2 = read.table("np2.dat",header=T)
npxlab = "Number hours/week"
npylab = "Number sections"
#make image/contour plot
library(akima)
np2.smooth = with(np2, interp(time, sections, sqrt(count), x0=seq(0,7,length=60), y0=seq(0,7,length=60)))
View(np2)
set.seed(12345)
fit=kmeans(np2,5,100,100)
names(fit)
kmeans()?
kmeans?
set.seed(12345)
#kmeans(data; # of clusters; max iterations; if centers is a number, how many random sets should be chosen?)
fit=kmeans(np2,5,100,100)
names(fit)
fit$cluster[1:10]
#cluster centers
fit$centers
summary(fit)
set.seed(12345)
#kmeans(data; # of clusters; max iterations; if centers is a number, how many random sets should be chosen?)
fit=kmeans(np2,5,100,100)
names(fit)
#cluster assignments
fit$cluster[1:10]
table(fit$cluster)
fit$withinss
sum(fit$withinss)
plot(jitter(np2$time), jitter(np2$sections),
col=fit$cluster, pch=16, cex=.5,
xlab=npxlab, ylab=npylab)
points(fit$centers,pch=3,cex=4)
#example of step4. quantile method
x=(as.integer(runif(20)*20))
x
#example of step4. quantile method
x=(as.integer(runif(20)*20))
cut(x, quantile(x, probs=seq(0,1,by=.2)), labels=-2:2, include.lowest=T)
theater = read.csv("theater.csv")
View(theater)
theater = read.csv("theater.csv")
theater$age2 = theater$age*5 + 20 # transform response cat to age in yrs
Ztheater = data.frame(lapply(theater[,1:5], scale, scale=T)) # standardize data
theater = read.csv("theater.csv")
theater$age2 = theater$age*5 + 20 # transform response cat to age in yrs
Ztheater = data.frame(lapply(theater[,1:5], scale, scale=T)) # standardize data
View(theater)
View(Ztheater)
View(Ztheater)
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
theater = read.csv("theater.csv")
theater$age2 = theater$age*5 + 20 # transform response cat to age in yrs
ztheater = data.frame(lapply(theater[,1:5], scale, scale=T)) # standardize data
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
summary(fit.th3)
summary.kmeans = function(fit)
{
p = ncol(fit$centers)
k = nrow(fit$centers)
n = sum(fit$size)
sse = sum(fit$withinss)
xbar = t(fit$centers)%*%fit$size/n
ssb = sum(fit$size*(fit$centers - rep(1,k) %*% t(xbar))^2)
print(data.frame(
n=c(fit$size, n),
Pct=(round(c(fit$size, n)/n,2)),
round(rbind(fit$centers, t(xbar)), 2),
RMSE = round(sqrt(c(fit$withinss/(p*fit$size-1), sse/(p*(n-k)))), 4)
))
cat("SSE = ", sse, "; SSB = ", ssb, "\n")
cat("R-Squared = ", ssb/(ssb+sse), "\n")
cat("Pseudo F = ", (ssb/(k-1))/(sse/(n-k)), "\n\n");
invisible(list(sse=sse, ssb=ssb, Rsqr=ssb/(ssb+sse), F=(ssb/(k-1))/(sse/(n-k))))
}
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
summary(fit.th3)
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
summary.kmeans(fit.th3)
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
summary(fit.th3)
set.seed(12345)
fit.th3 = kmeans(ztheater, 3,nstart=100)
summary(fit.th3)
