cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
# read dependent variable
y = read.csv("data/booktrain.csv")
head(y)
#merge data
all = left_join(x,y, by="id")
dim(all)
fit = lm(logtarg ~ log(f +1), all) #question: why add 1 to f here?
summary(fit)
train = all[all$logtarg!=0,]
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
names(all)
View(all)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #average price per ordered item
)%>%
select(id, recency_first, recency_last, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
x$avg_ord = x$monetary_tot/x$frequency_ord
cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
# read dependent variable
y = read.csv("data/booktrain.csv")
head(y)
#merge data
all = left_join(x,y, by="id")
dim(all)
fit = lm(logtarg ~ log(f +1), all) #question: why add 1 to f here?
summary(fit)
train = all[all$logtarg!=0,]
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
names(all)
View(all)
View(x)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #average price per ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
x$avg_ord = x$monetary_tot/x$frequency_ord
cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
#merge data
all = left_join(x,y, by="id")
dim(all)
fit = lm(logtarg ~ log(f +1), all) #question: why add 1 to f here?
summary(fit)
train = all[all$logtarg!=0,]
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
names(all)
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration, all)
summary(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first, all)
summary(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
summary(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
names(all)
all$yhat = predict(fit1,all)
length(all$yhat)
head(all)
test = is.na(all$logtarg)
out = all[test,c(1,11)]
head(out)
#write.csv(all[test, c(1,4)], "output/test_rfm.csv", row.names=F)
View(out)
all$yhat = predict(fit1,all)
length(all$yhat)
head(all)
test = is.na(all$logtarg)
out = all[test,c(1,12)]
head(out)
#write.csv(all[test, c(1,4)], "output/test_rfm.csv", row.names=F)
View(out)
all$yhat = predict(fit1,all)
length(all$yhat)
head(all)
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
#write.csv(all[test, c(1,4)], "output/test_rfm.csv", row.names=F)
View(all)
all$yhat = predict(fit1,all)
length(all$yhat)
head(all)
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
write.csv(out, "output/test_jc.csv", row.names=F)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/siliangchen/msia421_hw1_kaggle")
library(ggplot2)
library(lubridate)
library(dplyr)
#read orders
dat = read.csv("data/orders.csv")
dat$orddt = as.Date(dat$orddate, "%d%b%Y")
head(dat)
dat$orddate = NULL
str(dat)
dim(dat)
View(dat)
#min date = "2007-11-04"
min(dat$orddt)
#max date = "2014-07-31"
max(dat$orddt)
# read dependent variable
y = read.csv("data/booktrain.csv")
head(y)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #average price per ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
x$avg_ord = x$monetary_tot/x$frequency_ord
cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
#merge data
all = left_join(x,y, by="id")
dim(all)
#merge data
all = left_join(x,y, by="id")
dim(all)
fit = lm(logtarg ~ log(f +1), all) #question: why add 1 to f here?
summary(fit)
train = all[all$logtarg!=0,]
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
View(train)
train = all %>%
filter(!is.na(logtarg))
#[all$logtarg!=0,]
#is.na(all$logtarg)
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
train = all[!is.na(logtarg)]
train = all[!is.na(all$logtarg)]
train = all[!is.na(all$logtarg),]
#is.na(all$logtarg)
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
View(train)
View(y)
test = merge(x,y,by='id',test.x = TRUE)
#merge data
all = left_join(x,y, by="id")
dim(all)
all2 = merge(x,y,by="id",all.x = TRUE)
dim(all2)
8311-8224
View(train)
missing = dat[!complete.cases(dat),]
missing
View(dat)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price) + geom_line()
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_line()
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_histogram(binwidth = 1)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_histogram(binwidth = 30)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_histogram(binwidth = 100)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_histogram(binwidth = 30)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_histogram(binwidth = 300)
missing = dat[!complete.cases(dat),] #no missing value
plot(dat$price)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price) + geom_histogram(binwidth = 30)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=price)) + geom_histogram(binwidth = 30)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=qty)) + geom_histogram(binwidth = 1)
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
View(dat)
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
#summary(fit1)
vif(f1)
library(ggplot2)
library(lubridate)
library(dplyr)
library(car)
#read orders
dat = read.csv("data/orders.csv")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/siliangchen/msia421_hw1_kaggle")
#read orders
dat = read.csv("data/orders.csv")
dat$orddt = as.Date(dat$orddate, "%d%b%Y")
head(dat)
dat$orddate = NULL
str(dat)
dim(dat)
#min date = "2007-11-04"
min(dat$orddt)
#max date = "2014-07-31"
max(dat$orddt)
missing = dat[!complete.cases(dat),] #no missing value
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
View(dat)
max(dat$qty)
summary(dat)
View(all)
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#if category = 99, book = 0, otherwise book = 1
dat[price==0,]
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#if category = 99, book = 0, otherwise book = 1
dat[dat$price==0,]
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#if category = 99, book = 0, otherwise book = 1
price0 = dat[dat$price==0,]
View(price0)
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#if category = 99, book = 0, otherwise book = 1
price0 = dat[dat$price==0,]
table(price0$category,price0$price)
12563/33355
len(dat[dat$price == 0,])/len(dat)
len(dat)
count(dat)
count(dat[dat$price == 0,])/count(dat)
count(dat[dat$price == 0,])
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#if category = 99, book = 0, otherwise book = 1
price0 = dat[dat$price==0,]
table(price0$category,price0$price)
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add new feature: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add new feature: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
View(dat)
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add new feature: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
# read dependent variable
y = read.csv("data/booktrain.csv")
head(y)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f,book)
View(dat)
# do simple roll up
x = dat %>%
group_by(id,book) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f,book)
head(x)
dim(x)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f,book)
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
# read dependent variable
y = read.csv("data/booktrain.csv")
head(y)
# do simple roll up
x = dat %>%
group_by(id) %>%
summarise(f=n(),
recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase
recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase
date_duration = recency_last - recency_first, #time between 1st and last purchases
frequency_qty = sum(qty), #number of items
frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
monetary_tot = sum(price * qty), #total spent
monetary_avg = mean(price) #how expensive is each ordered item
)%>%
select(id, recency_first, recency_last, date_duration, frequency_qty, frequency_ord, monetary_tot, monetary_avg,f)
head(x)
dim(x)
x$avg_ord = x$monetary_tot/x$frequency_ord
cor_mat = cor(x[-1])
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected
fit = lm(logtarg ~ log(f +1), all) #question: why add 1 to f here?
summary(fit)
names(all)
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311
plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(sqrt(train$frequency_ord), train$logtarg)
plot(sqrt(train$frequency_qty), train$logtarg)
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(monetary_avg) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
#summary(fit1)
vif(f1)
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
summary(fit1)
#vif(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + date_duration + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + ecency_first + recency_last, all)
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
fit1 = lm(logtarg ~ log(monetary_tot+1) + log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
all$yhat = predict(fit1,all)
length(all$yhat)
head(all)
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
write.csv(out, "output/test_jc.csv", row.names=F)
missing = dat[!complete.cases(dat),] #no missing value
#category frequency
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)
summary(dat)
#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0, otherwise book = 1
dat$book = 0
dat$book[dat$category!=99]=1
table(dat$book)
# read in dependent variable
y = read.csv("data/booktrain.csv")
head(y)
# read in dependent variable
y = read.csv("data/booktrain.csv")
#head(y)
#join tables
all = left_join(x,y,by="id")
dim(all)
# read in dependent variable
y = read.csv("data/booktrain.csv")
#head(y)
#join tables
all = left_join(x,y,by="id")
dim(all)
fit = lm(logtarg ~ log(f +1), all) #question: why add 1 to f here?
summary(fit)
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
#note: need to do some data cleaning on customers in the training set with order history price = $0
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + sqrt(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
all$yhat = predict(fit1,all)
length(all$yhat)
head(all)
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
write.csv(out, "output/test_jc.csv", row.names=F)
