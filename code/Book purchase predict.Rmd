---
title: "book purchase prediction"
author: "Jamie Chen"
date: "1/13/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = "/Users/siliangchen/msia421_hw1_kaggle")
knitr::opts_knit$set(root.dir = "/Users/spencermoon/Documents/MSiA_421/msia421_hw1_kagglee")
```

```{r, message = FALSE}
library(ggplot2)
library(lubridate)
library(MASS)
library(dplyr)
library(car)
```

1. Reading data
```{r}
#read orders
dat = read.csv("../data/orders.csv")
dat$orddt = as.Date(dat$orddate, "%d%b%Y")
head(dat)
dat$orddate = NULL
str(dat)
dim(dat)
```

```{r}
#min date = "2007-11-04"
min(dat$orddt)
#max date = "2014-07-31"
max(dat$orddt)
```

1.2 EDA and data checks on orders
a. qualitative var - category => category 99 has some oddities (most qty with price - $0, max price = 1533)
b. quantitative vars - qty,price
```{r}

missing = dat[!complete.cases(dat),] #no missing value

#a.category frequency 
ggplot(data=dat, aes(x=category)) + geom_histogram(binwidth = 1)

#category by price (most $)
result = tapply(dat$price, dat$category,mean)
sort_price = result[order(result)] #category 17 (art prints) is $$$
art_collect = dat[dat$category==17,] #these people buy at most 3 items

#category by Q (most popular category)
result2 = tapply(dat$qty, dat$category,mean)
sort_q = result2[order(result2)] #note - cat99 nonbooks has the highest avg 
result3 = tapply(dat$qty, dat$category,median)
sort_q3 = result3[order(result3)] #median is all 1
result4 = tapply(dat$qty, dat$category,max)
sort_q4 = result4[order(result4)] #category 99 is nonbooks, ID 8070857 has price =0, Q = max.

#category by price * Q (most popular category)
qp = tapply(dat$qty * dat$price, dat$category,mean)
sort_qp = qp[order(qp)] #align with expectation - 17 has the largest avg order size

qpm = tapply(dat$qty * dat$price, dat$category,max)
sort_qpm = qpm[order(qpm)] #8,14,35,37 have the max one-time order amounts, >$140k; makes sense, 37 is learning=> textbooks
```

```{r}
#Descriptive stats
summary(dat[,-1])

#investigate items with $0 in price
percent_price0 = count(dat[dat$price == 0,])/count(dat)
#The majority of items with 0 price are non-books, add flag to indicate: if category = 99, book = 0
dat$book = 0
dat$book[dat$category!=99]=1 
table(dat$book)
```

1.3 feature engineer
-recency: max/min time since last purchase, indicates inactivity
-frequency: count of previous behaviors, indicates loyalty
-monetary: sum/total spend of $ or time over a past period
-time of file: time since first purchase (min/max) 
```{r}
# do simple roll up
x = dat %>%
 group_by(id) %>%
 summarise(f=n(),
           # FEATURES ADDED BY JAMIE
           recency_first = as.numeric(as.Date('2014-08-01') - min(orddt)), #time since first purchase - tof
           recency_last = as.numeric(as.Date('2014-08-01') - max(orddt)), #time since last purchase - rec
           date_duration = recency_first - recency_last, #time between 1st and last purchases
           p_qty = sum(qty), #number of items 
           frequency_ord = n_distinct(ordnum), #number of distinct orders, which <= f
           monetary_tot = sum(price * qty), #total spent
           monetary_avg = mean(price), #how expensive is each ordered item
           
           # FEATURES ADDED BY SPENCER
           count_cat = n_distinct(category) #number of distinct categories ordered
          )%>% 
  dplyr::select(id, recency_first, recency_last, date_duration, p_qty, frequency_ord, monetary_tot, monetary_avg, count_cat,f)

head(x)
dim(x)
```
Additional features
```{r}
# ADDED BY JAMIE
#avg order size
x$avg_ord = x$monetary_tot/x$frequency_ord 

#purchase rate = purchases/period
x$prate = x$frequency_ord/x$recency_first

# ADDED BY SPENCER
#diversity of order
x$catrate = x$count_cat/x$frequency_ord
x$prate2 = x$frequency_ord/(x$date_duration + 1)

#check predictors cor
cor_mat = cor(x[-1]) 
cor(x[-1]) > 0.6 #f & freq_ord are colinear as expected, avg_ord and monetary_tot
```
2.Model fitting
```{r}
# read in dependent variable
y = read.csv("../data/booktrain.csv")
#head(y)

#join tables
all = left_join(x,y,by="id")
dim(all)
```

Variable transformation based on EDA
- Create log transormation for F and M because of right skew
```{r}
train = all[!is.na(all$logtarg),] #8224 obs instead of 8311

plot(log(train$monetary_tot), train$logtarg)
plot(log(train$monetary_tot +1), train$logtarg)
plot(train$monetary_tot, train$logtarg)

plot(log(train$monetary_avg), train$logtarg)
plot(log(train$avg_ord), train$logtarg)
plot(log(train$frequency_ord), train$logtarg)

plot((train$p_qty), train$logtarg)
plot(log(train$p_qty), train$logtarg)

plot(log(train$prate),train$logtarg)
```
2.1 Baseline model => submitted with score 0.61844
```{r}
fit1 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + recency_first + recency_last, all)
summary(fit1)
vif(fit1)
plot(fit1)
```
2.2 Model fit2 => submitted with score 0.61887
```{r}
fit2 = lm(logtarg ~ log(monetary_avg+1) + log(avg_ord+1) + log(frequency_ord) + log(prate) + recency_first + recency_last, all)
summary(fit2)

vif(fit2)
```

2.3 Model fit3 ADDED BY SPENCER
```{r}
full = lm(logtarg ~ recency_first + recency_last + date_duration + log(p_qty) + log(count_cat)
          + log(catrate) + log(monetary_avg + 1)  + log(avg_ord + 1) 
          + log(frequency_ord) + log(prate)
        , data = train)
summary(full)


adj = step(full, scope = list(upper=full), data = train, direction="both")
summary(adj)
```




```{r}
#Predict and output
all$yhat = predict(adj,all)
length(all$yhat)
head(all)

#output test values
test = is.na(all$logtarg)
keep = c('id', 'yhat')
out = all[test,keep]
head(out)
write.csv(out, "../output/test_sm.csv", row.names=F)
```
